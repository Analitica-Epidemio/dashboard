"""
Validador simplificado y optimizado para archivos epidemiol√≥gicos.

Elimina redundancias y enfoca en validaciones cr√≠ticas.
"""

import logging
from datetime import datetime
from typing import Dict, Optional

import numpy as np
import pandas as pd

from .config import Columns
from .config.constants import (
    BOOLEAN_COLUMNS,
    BOOLEAN_MAPPING,
    DATE_COLUMNS,
    DOCUMENTO_MAPPING,
    NULL_VALUES,
    NUMERIC_COLUMNS,
    PROVINCIA_MAPPING,
    REQUIRED_COLUMNS,
    SEXO_MAPPING,
    UPPERCASE_COLUMNS,
    VALIDATION_LIMITS,
)

logger = logging.getLogger(__name__)


class OptimizedDataValidator:
    """
    Validador de datos optimizado para m√°ximo rendimiento.

    Caracter√≠sticas:
    - Validaci√≥n vectorizada con pandas
    - Limpieza de datos en una pasada
    - Reportes estructurados de errores
    - Enfoque en validaciones cr√≠ticas solamente
    """

    def __init__(self):
        """Inicializa el validador sin dependencias externas."""
        self.validation_report = {"errors": [], "warnings": [], "stats": {}}

    def validate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Valida y limpia un batch de datos.

        Args:
            df: DataFrame de datos a validar

        Returns:
            DataFrame validado y limpio
        """
        logger.info(f"Validando batch de {len(df)} registros")

        original_count = len(df)

        # 1. Validaci√≥n estructural cr√≠tica
        self._validate_structure(df)

        # 2. Limpieza de datos vectorizada
        df = self._clean_data_vectorized(df)

        # 3. Conversi√≥n de tipos optimizada
        df = self._convert_types_vectorized(df)

        # 4. Validaciones cr√≠ticas solamente
        self._validate_critical_values(df)

        # 5. Actualizar estad√≠sticas
        self._update_validation_stats(original_count, len(df))

        logger.info(
            f"Validaci√≥n completada: {len(df)}/{original_count} registros v√°lidos"
        )

        return df

    def _validate_structure(self, df: pd.DataFrame) -> None:
        """Valida estructura cr√≠tica del DataFrame."""
        # Verificar columnas cr√≠ticas solamente
        missing_critical = [col for col in REQUIRED_COLUMNS if col not in df.columns]

        if missing_critical:
            # DEBUGGING EXHAUSTIVO
            logger.error(f"üìã Columnas presentes en CSV ({len(df.columns)}): {list(df.columns[:20])}...")
            logger.error(f"‚ùå Columnas cr√≠ticas faltantes ({len(missing_critical)}): {missing_critical}")
            logger.error(f"üîç Tipo de REQUIRED_COLUMNS: {type(REQUIRED_COLUMNS)}")
            logger.error(f"üîç Tipo de df.columns: {type(df.columns)}")
            logger.error(f"üîç Primer elemento REQUIRED_COLUMNS: {REQUIRED_COLUMNS[0]} (tipo: {type(REQUIRED_COLUMNS[0])})")
            logger.error(f"üîç Primer elemento df.columns: {df.columns[0]} (tipo: {type(df.columns[0])})")

            # Verificar manualmente las 3 cr√≠ticas
            for col_name in missing_critical[:3]:
                logger.error(f"üîç '{col_name}' in df.columns: {col_name in df.columns}")
                logger.error(f"üîç B√∫squeda manual: {[c for c in df.columns if col_name in c]}")

            error_msg = f"Columnas cr√≠ticas faltantes: {missing_critical}"
            self.validation_report["errors"].append(
                {
                    "type": "missing_critical_columns",
                    "columns": missing_critical,
                    "severity": "critical",
                }
            )

            raise ValueError(error_msg)

        # Verificar que no est√© vac√≠o
        if df.empty:
            error_msg = "DataFrame vac√≠o"
            self.validation_report["errors"].append(
                {"type": "empty_dataframe", "severity": "critical"}
            )

            raise ValueError(error_msg)

        logger.debug(f"Estructura v√°lida: {len(df.columns)} columnas, {len(df)} filas")

    def _clean_data_vectorized(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Limpieza vectorizada de datos para m√°ximo rendimiento.

        Aplica todas las limpiezas en una sola pasada usando pandas vectorization.
        """
        logger.debug("Iniciando limpieza vectorizada")

        # Crear copia para evitar SettingWithCopyWarning
        df = df.copy()

        # 1. Limpiar strings: strip espacios
        # IMPORTANTE: NO usar .astype(str) porque convierte NaN ‚Üí "nan" string
        # En su lugar, solo limpiar valores que ya son strings
        string_cols = df.select_dtypes(include=["object"]).columns
        for col in string_cols:
            # Solo aplicar strip a valores no-nulos para preservar NaN originales
            df[col] = df[col].str.strip()

        # 2. Reemplazar valores nulos en una operaci√≥n
        df = df.replace(list(NULL_VALUES), np.nan)

        # 3. Normalizar campos espec√≠ficos a may√∫sculas
        for col in UPPERCASE_COLUMNS:
            if col in df.columns:
                df[col] = df[col].str.upper()

        # 4. Normalizar provincias usando mapeo
        provincia_cols = [col for col in df.columns if "PROVINCIA" in col]
        for col in provincia_cols:
            if col in df.columns:
                df[col] = df[col].replace(PROVINCIA_MAPPING)

        # 5. Normalizar tipos de documento
        if Columns.TIPO_DOC.name in df.columns:
            df[Columns.TIPO_DOC.name] = df[Columns.TIPO_DOC.name].replace(
                {k: v.name for k, v in DOCUMENTO_MAPPING.items()}
            )

        # 6. Normalizar sexo
        if Columns.SEXO.name in df.columns:
            df[Columns.SEXO.name] = df[Columns.SEXO.name].replace(
                {k: v.name for k, v in SEXO_MAPPING.items()}
            )

        logger.debug("Limpieza vectorizada completada")

        return df

    def _convert_types_vectorized(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Conversi√≥n de tipos vectorizada usando pandas.

        OPTIMIZACI√ìN CR√çTICA:
        - Los tipos num√©ricos YA vienen como Int64 desde read_csv(dtype={'col': 'Int64'})
        - Las fechas YA son datetime64[ns] desde read_csv(parse_dates=...)
        - Solo necesitamos convertir BOOLEANOS (leyeron como str, necesitan mapping)
        """
        logger.debug("Iniciando conversi√≥n de tipos (solo booleanos)")

        # Solo convertir booleanos (√∫nico tipo que requiere mapeo)
        for col in BOOLEAN_COLUMNS:
            if col in df.columns:
                # Crear mapeo para booleanos
                bool_map = {}
                for k, v in BOOLEAN_MAPPING.items():
                    bool_map[k] = v
                    bool_map[str(v).upper()] = v  # Tambi√©n mapear 'TRUE'/'FALSE'
                    bool_map[str(int(v))] = v  # Tambi√©n mapear '1'/'0'

                df[col] = df[col].astype(str).str.upper().map(bool_map)

        logger.debug("Conversi√≥n de booleanos completada")

        return df

    def _validate_critical_values(self, df: pd.DataFrame) -> None:
        """
        Valida solo los valores cr√≠ticos que pueden causar fallos.

        Enfoque minimalista: solo lo que realmente importa.
        """
        # 1. Verificar IDs √∫nicos cr√≠ticos
        if Columns.IDEVENTOCASO.name in df.columns:
            duplicated_ids = df[Columns.IDEVENTOCASO.name].duplicated()
            if duplicated_ids.sum() > 0:
                duplicate_count = duplicated_ids.sum()
                self.validation_report["warnings"].append(
                    {
                        "type": "duplicate_event_ids",
                        "count": duplicate_count,
                        "severity": "high",
                    }
                )

        # 2. Verificar rangos cr√≠ticos de edad solamente
        age_cols = ["EDAD_ACTUAL", "EDAD_DIAGNOSTICO"]
        for col in age_cols:
            if col in df.columns:
                invalid_ages = (df[col] < VALIDATION_LIMITS["min_age"]) | (
                    df[col] > VALIDATION_LIMITS["max_age"]
                )
                invalid_count = invalid_ages.sum()

                if invalid_count > 0:
                    self.validation_report["warnings"].append(
                        {"type": "invalid_ages", "column": col, "count": invalid_count}
                    )

        # 3. Verificar fechas futuras solo en fechas cr√≠ticas
        current_date = pd.Timestamp.now()
        critical_date_cols = ["FECHA_APERTURA", "FECHA_NACIMIENTO"]

        for col in critical_date_cols:
            if col in df.columns and df[col].dtype == "datetime64[ns]":
                future_dates = df[col] > current_date
                future_count = future_dates.sum()

                if future_count > 0:
                    self.validation_report["warnings"].append(
                        {"type": "future_dates", "column": col, "count": future_count}
                    )

        logger.debug("Validaci√≥n de valores cr√≠ticos completada")

    def _update_validation_stats(self, original_count: int, final_count: int) -> None:
        """Actualiza estad√≠sticas de validaci√≥n."""
        self.validation_report["stats"] = {
            "original_rows": original_count,
            "valid_rows": final_count,
            "rows_removed": original_count - final_count,
            "errors_count": len(self.validation_report["errors"]),
            "warnings_count": len(self.validation_report["warnings"]),
            "validation_time": datetime.utcnow().isoformat(),
        }

    def get_validation_report(self) -> Dict:
        """Obtiene el reporte de validaci√≥n completo."""
        return self.validation_report

    def validate_input(self, df: pd.DataFrame) -> bool:
        """Valida entrada del procesador."""
        return df is not None and not df.empty


class SchemaValidator:
    """
    Validador de esquema independiente para verificaci√≥n r√°pida.

    √ötil para validar archivos antes del procesamiento completo.
    """

    @staticmethod
    def validate_csv_schema(df: pd.DataFrame) -> Dict[str, any]:
        """
        Valida esquema CSV de forma r√°pida.

        Returns:
            Diccionario con resultado de validaci√≥n
        """
        result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "stats": {
                "total_columns": len(df.columns),
                "total_rows": len(df),
                "required_columns_found": 0,
                "match_percentage": 0.0,
            },
        }

        # Verificar columnas requeridas
        missing_columns = []
        found_columns = 0

        for col in REQUIRED_COLUMNS:
            if col in df.columns:
                found_columns += 1
            else:
                missing_columns.append(col)

        result["stats"]["required_columns_found"] = found_columns
        result["stats"]["match_percentage"] = (
            found_columns / len(REQUIRED_COLUMNS)
        ) * 100

        if missing_columns:
            result["is_valid"] = False
            result["errors"].append(
                {
                    "type": "missing_required_columns",
                    "columns": missing_columns,
                    "count": len(missing_columns),
                }
            )

        # Verificar filas vac√≠as
        if df.empty:
            result["is_valid"] = False
            result["errors"].append(
                {"type": "empty_file", "message": "El archivo no contiene datos"}
            )

        # Warnings por columnas esperadas faltantes (no cr√≠ticas)
        expected_columns = set(DATE_COLUMNS + NUMERIC_COLUMNS + BOOLEAN_COLUMNS)
        actual_columns = set(df.columns)
        missing_optional = expected_columns - actual_columns

        if missing_optional:
            result["warnings"].append(
                {
                    "type": "missing_optional_columns",
                    "columns": list(missing_optional),
                    "count": len(missing_optional),
                    "message": f"Faltan {len(missing_optional)} columnas opcionales",
                }
            )

        return result

    @staticmethod
    def get_schema_summary(df: pd.DataFrame) -> Dict[str, any]:
        """Obtiene resumen r√°pido del esquema."""
        return {
            "filename": "CSV_INPUT",
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024 * 1024),
            "column_types": {
                "object": len(df.select_dtypes(include=["object"]).columns),
                "numeric": len(df.select_dtypes(include=[np.number]).columns),
                "datetime": len(df.select_dtypes(include=["datetime64"]).columns),
                "boolean": len(df.select_dtypes(include=["bool"]).columns),
            },
            "null_percentage": (df.isnull().sum().sum() / (len(df) * len(df.columns)))
            * 100,
        }
